{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, Lasso\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean, absolute, sqrt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('train.csv', index_col = \"Id\")\n",
    "#dataset_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(dataset_train.shape)\n",
    "print(dataset_train.columns)\n",
    "\n",
    "# The training dataset includes 1460 samples, 79 features, and 1 response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = dataset_train.iloc[:,79]\n",
    "plt.hist(price)\n",
    "print(\"\\u03BB = \", round(price.skew(),4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(price))\n",
    "print(\"\\u03BB = \", round(np.log(price).skew(),4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap\n",
    "import seaborn as sns\n",
    "train_data = dataset_train.iloc[:,1:79]\n",
    "train_data.insert(0,\"SalePrice\",dataset_train.iloc[:,79],True)\n",
    "\n",
    "\n",
    "all_numVar = train_data.loc[:,train_data.dtypes != object]\n",
    "cor_numVar = all_numVar.corr()\n",
    "\n",
    "cor_numVar = cor_numVar.sort_values(by='SalePrice', ascending = False, axis=1)\n",
    "cor_numVar = cor_numVar.sort_values(by='SalePrice', ascending = False, axis=0)\n",
    "\n",
    "\n",
    "cor_numVar = cor_numVar.iloc[:10,:10]\n",
    "sns.heatmap(cor_numVar, cmap='coolwarm', fmt='.2f', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = dataset_train[\"SalePrice\"]\n",
    "#all = pd.concat([dataset_train, dataset_test]).iloc[:,0:78]\n",
    "print(dataset_train.dtypes[0:2])\n",
    "#second column MSSubClass is categorical feature but is considered as \n",
    "#int so we remapped it\n",
    "\n",
    "dataset_train[\"MSSubClass\"] = dataset_train[\"MSSubClass\"].astype(str)\n",
    "dataset_train[\"MSSubClass\"] = dataset_train[\"MSSubClass\"].replace([\"20\",\"30\",\"40\",\"45\",\"50\",\"60\",\"70\",\"75\",\"80\",\"85\",\"90\",\"120\",\"150\"\n",
    "                           , \"160\",\"180\",\"190\"], [\"1-story_1946+\",\"1-story_1945-\", \"1-story_with_finished_attic\"\n",
    "                           ,\"1.5_story_unifinished\", \"1.5_finished\", \"2-story_1946+\", \"2-story_1945-\",\n",
    "                            \"2.5_story\", \"split_or_multi_level\", \"split_foyer\", \"1-story_PUD\",  \"duplex\",\n",
    "                            \"1.5_story_PUD\", \"2-story_PUD\", \"PUD_Multilevel\",\"2_family_conversion\"])\n",
    "dataset_train[\"MSSubClass\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify yrs\n",
    "#YearBuilt: Original construction date\n",
    "#YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n",
    "#YrSold: Year Sold (YYYY)\n",
    "\n",
    "# remodel 0 = no 1 =yes\n",
    "# age: yr-sold - max(yearbulit, yearromodadd)\n",
    "\n",
    "remod = dataset_train[\"YearBuilt\"] == dataset_train[\"YearRemodAdd\"]\n",
    "\n",
    "\n",
    "remod = np.multiply(np.array(remod),1)\n",
    "        \n",
    "#HouseAge = all[\"YearBuilt\"] - max(all[\"YearBuilt\"], all[\"YearRemodAdd\"])\n",
    "\n",
    "HouseAge = dataset_train[\"YrSold\"] - dataset_train[[\"YearBuilt\", \"YearRemodAdd\"]].max(axis=1)\n",
    "print(min(HouseAge))\n",
    "\n",
    "dataset_train[\"HouseAge\"] = HouseAge\n",
    "dataset_train[\"remod\"] = remod\n",
    "dataset_train = dataset_train.drop(columns=[\"YearBuilt\",\"YearRemodAdd\",\"YrSold\" ])\n",
    "dataset_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BsmtFullBath: Basement full bathrooms\n",
    "#BsmtHalfBath: Basement half bathrooms\n",
    "#FullBath: Full bathrooms above grade\n",
    "#HalfBath: Half baths above grade\n",
    "\n",
    "#A full bathroom is made up of four parts: a sink, a shower, a bathtub, and a toilet. Anything less than that, and you can't officially consider it a full bath\n",
    "#A bathroom with just a sink and a toilet is a half-bath.\n",
    "\n",
    "dataset_train['BsmtFullBath']  = dataset_train['BsmtFullBath'].fillna(0)\n",
    "dataset_train['BsmtHalfBath']  = dataset_train['BsmtHalfBath'].fillna(0)\n",
    "\n",
    "dataset_train['NumBath'] = dataset_train['BsmtFullBath'] + dataset_train['BsmtHalfBath'] *.5 + dataset_train['HalfBath'] + dataset_train['HalfBath'] * .5\n",
    "\n",
    "dataset_train = dataset_train.drop(columns=['BsmtFullBath','BsmtHalfBath','HalfBath','HalfBath' ])\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#garage variables\n",
    "\n",
    "#price\n",
    "\n",
    "dataset_train['GarageArea']  = dataset_train['GarageArea'].fillna(0)\n",
    "dataset_train['GarageCars']  = dataset_train['GarageCars'].fillna(0)\n",
    "\n",
    "cor_Garage_area_cars = dataset_train['GarageArea'].corr(dataset_train['GarageCars'])\n",
    "print(\"Since GarageArea and GarageCars are highly correlated (cor = \", round(cor_Garage_area_cars,4), \")\\n\")\n",
    "print(\"in case of parsimony, we choose one that has a higher correlation with SalePrice\")\n",
    "\n",
    "print(\"correlation between GarageArea and SalePrice is\", round(dataset_train['GarageArea'][0:1459].corr(price),4))\n",
    "print(\"correlation between GarageCars and SalePrice is\", round(dataset_train['GarageCars'][0:1459].corr(price),4))\n",
    "print(\"Therefore we keep the GarageCars variable\")\n",
    "\n",
    "dataset_train = dataset_train.drop(columns=['GarageArea' ])\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1stFlrSF and totalbasement\n",
    "#x1stF   + 2nd = grlivarea\n",
    "\n",
    "df_test1 = dataset_train[['1stFlrSF', '2ndFlrSF', 'GrLivArea']]\n",
    "df_test1['sum1and2'] = df_test1['1stFlrSF'] + df_test1['2ndFlrSF']\n",
    "df_test1['DiffSumAndTot'] = df_test1['GrLivArea'] - df_test1['sum1and2'] \n",
    "print(df_test1[1:20])\n",
    "\n",
    "print(\"we can see there are only\", sum((dataset_train['1stFlrSF'] + dataset_train['2ndFlrSF']) != dataset_train['GrLivArea']) ,\"out of 2919 data sets, approximately 1% of total data set\\n\",\n",
    "      \"that first floor area + second floor area != totol above ground living area, probably there are some houses has\\n\",\n",
    "      \"> 2 floors that has not been taken into account of the dataset\")\n",
    "print(\"so we comes to the conclusion that Total above ground living area('GrLivArea') explains both two variables\\n\")\n",
    "print(\"first floor sf and second floor sq\")\n",
    "print(\"so we drop those two features\")\n",
    "\n",
    "dataset_train = dataset_train.drop(columns=['1stFlrSF','2ndFlrSF' ])\n",
    "\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"since Total # of rooms and above ground living area are highly correlated, cor = \", round(dataset_train['TotRmsAbvGrd'].corr(dataset_train['GrLivArea']),4))\n",
    "print(\"so we choose one has higher correlation with SalePrice\")\n",
    "dataset_train = dataset_train.drop(columns=['TotRmsAbvGrd' ])\n",
    "dataset_train.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_train.iloc[0:1460,1:71]\n",
    "#train_data.insert(0,\"SalePrice\",price,True)\n",
    "numericVars = dataset_train.dtypes != 'O' #find numeric vars\n",
    "\n",
    "all_numVar = train_data.loc[:,train_data.dtypes != object]\n",
    "cor_numVar = all_numVar.corr()\n",
    "\n",
    "\n",
    "cor_numVar = cor_numVar.sort_values(by='SalePrice', ascending = False, axis=1)\n",
    "cor_numVar = cor_numVar.sort_values(by='SalePrice', ascending = False, axis=0)\n",
    "\n",
    "\n",
    "cor_numVar = cor_numVar.iloc[:10,:10]\n",
    "sns.heatmap(cor_numVar, cmap='coolwarm', fmt='.2f', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all missing values by the corresponding featureâ€™s mean. \n",
    "# Then, to put all features on a common scale, \n",
    "# we standardize the data by rescaling features to zero mean and unit variance\n",
    "# Intuitively, we standardize the data for two reasons. \n",
    "# First, it proves convenient for optimization. \n",
    "# Second, because we do not know a priori which features will be relevant,\n",
    "# we do not want to penalize coefficients assigned to one feature \n",
    "# more than on any other.\n",
    "\n",
    "\n",
    "# If test data were inaccessible, mean and standard deviation could be\n",
    "# calculated from training data\n",
    "numeric_features = dataset_train.dtypes[dataset_train.dtypes != 'object'].index\n",
    "dataset_train[numeric_features] = dataset_train[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "# After standardizing the data all means vanish, hence we can set missing\n",
    "# values to 0\n",
    "dataset_train[numeric_features] = dataset_train[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we deal with discrete values. \n",
    "# We replace them by a one-hot encoding\n",
    "# Dummy variables\n",
    "# `Dummy_na=True` considers \"na\" (missing value) as a valid feature value, and\n",
    "# creates an indicator feature for it\n",
    "dataset_train = pd.get_dummies(dataset_train, dummy_na=True)\n",
    "dataset_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = all_features.iloc[1460:,]\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into categorical and continuous variables for analysis\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "df_cont = dataset_train.head().select_dtypes(include=numerics)\n",
    "df_cat = dataset_train.head().select_dtypes(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso for Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat([train, np.log(dataset_train['SalePrice'])], axis = 1)\n",
    "df = dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using lasso to eliminate some features\n",
    "X_lasso = df.loc[:, df.columns != 'SalePrice']\n",
    "y = df['SalePrice']\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=10)\n",
    "model = LassoCV(cv=cv)\n",
    "lassocv = model.fit(X_lasso, y)\n",
    "\n",
    "B_lasso = lassocv.coef_\n",
    "B_lasso = np.insert(B_lasso, 0, lassocv.intercept_, axis=0)\n",
    "\n",
    "print('The Coef are')\n",
    "print(B_lasso)\n",
    "#print('\\nlambda best is = %f' % lassocv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lasso =  df.loc[:, df.columns != 'SalePrice'] * lassocv.coef_\n",
    "df_lasso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lasso = df_lasso.loc[:, (df != 0).any(axis=0)]\n",
    "df_lasso.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([df_lasso, dataset_train['SalePrice']], axis = 1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Cross Validation\n",
    "\n",
    "def cross_validation (df, func):\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 10)\n",
    "    kf_rmse = []\n",
    "    \n",
    "    for train, test in kf.split(df):\n",
    "        X_train = df.iloc[train].loc[:, df.columns != 'SalePrice']\n",
    "        X_train = X_train.squeeze()\n",
    "        X_test = df.iloc[test].loc[:, df.columns != 'SalePrice']\n",
    "        y_train = df.iloc[train].loc[:,'SalePrice']\n",
    "        y_test = df.iloc[test].loc[:,'SalePrice']\n",
    "        \n",
    "        reg = func.fit(X_train, y_train)\n",
    "        y_hat = reg.predict(X_test)\n",
    "        \n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        kf_rmse.append(mean_squared_error(y_test, y_hat, squared=False))\n",
    "        \n",
    "    kf_RMSE = (1/10) * np.sum(kf_rmse)\n",
    "        \n",
    "    return (kf_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df1[\"SalePrice\"]\n",
    "X = df1.drop(\"SalePrice\", axis = 1)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which one produces higher R2 score and lower cv-rmse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "y_hat = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model_train_rmse = mean_squared_error(y, y_hat, squared=False)\n",
    "#score = model.score(x, y)\n",
    "#print(model_train_rmse)\n",
    "model_cv_rmse = cross_validation(df1,LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x = y, y = y_hat, s = 8, label = \"Test\")\n",
    "#plt.scatter(x = np.exp(y_train), y = np.exp(ols.predict(X_train)), s = 8, label = \"Train\")\n",
    "plt.plot([-2,8],[-2,8], color = \"r\")\n",
    "plt.legend(loc = 0)\n",
    "plt.title(\"Predicted Value vs True Value for Multiple Linear Regression\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Predicted Value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestRegressor(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_para_forest = {'n_estimators': [100,500,1000,2500,5000],\n",
    "                   'max_depth': [10,15,20,30,40,50],\n",
    "                   'max_features' : [5,7,15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_forest = GridSearchCV(randomForest, \n",
    "                                  grid_para_forest,\n",
    "                                  cv=10, n_jobs = 5, verbose = 1)\n",
    "grid_search_forest.fit(X, y)\n",
    "\n",
    "print(grid_search_forest.best_params_)\n",
    "print(grid_search_forest.best_score_)\n",
    "print(grid_search_forest.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_rf = grid_search_forest.best_estimator_\n",
    "pred_rf = bestmodel_rf.predict(X)\n",
    "#RMSLE = np.sqrt(mean_squared_log_error(pred_rf, y_test))\n",
    "#print(\"RMSLE: \" + str(round(RMSLE,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_importance = sorted(list(zip(X.columns, bestmodel_rf.feature_importances_)), key=lambda t:t[1], reverse=True)\n",
    "a, b = list(zip(*sorted_importance))\n",
    "plt.figure(figsize = (10,10))\n",
    "df = pd.DataFrame({'feature_name':a, 'importance_score':b})\n",
    "sns.barplot(data = df[0:9], x = 'importance_score', y= 'feature_name', orient = 'h');\n",
    "plt.title('Feature Importance Plot Random Forest')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "#plt.scatter(x = y_test, y = pred_rf, s = 8, label = \"Test\")\n",
    "plt.scatter(x = y, y = pred_rf, s = 8, label = \"Test\")\n",
    "#plt.scatter(x = y_train, y = bestmodel_rf.predict(X_train), s = 8, label = \"Train\")\n",
    "plt.plot([-2,8],[-2,8], color = \"r\")\n",
    "plt.legend(loc = 0)\n",
    "plt.title(\"Predicted Value vs True Value for Random Forest\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Predicted Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_train_rmse = mean_squared_error(y, pred_rf, squared=False)\n",
    "#score = model.score(x, y)\n",
    "#print(model_train_rmse)\n",
    "model2_cv_rmse = cross_validation(df1,bestmodel_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_para_gb = {'n_estimators': [100,500,1000,2500,5000],\n",
    "                   'learning_rate':[0.01,0.05,0.1],\n",
    "                   'max_depth':range(1,6),\n",
    "                   'max_features' : [5,7,15]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "grid_search_gb = GridSearchCV(gbm, grid_para_gb, cv=5, n_jobs = 5, verbose = 1)\n",
    "grid_search_gb.fit(X, y)\n",
    "\n",
    "print(grid_search_gb.best_params_)\n",
    "print(grid_search_gb.best_score_)\n",
    "print(grid_search_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_gb = grid_search_gb.best_estimator_\n",
    "pred_gb = bestmodel_gb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "#plt.scatter(x = y_test, y = pred_rf, s = 8, label = \"Test\")\n",
    "plt.scatter(x = y, y = pred_gb, s = 8, label = \"Test\")\n",
    "#plt.scatter(x = y_train, y = bestmodel_rf.predict(X_train), s = 8, label = \"Train\")\n",
    "plt.plot([-2,8],[-2,8], color = \"r\")\n",
    "plt.legend(loc = 0)\n",
    "plt.title(\"Predicted Value vs True Value for Gradient Boost\")\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Predicted Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_train_rmse = mean_squared_error(y, pred_gb, squared=False)\n",
    "score = model.score(X, y)\n",
    "print(score)\n",
    "model3_cv_rmse = cross_validation(df1,bestmodel_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['LinReg', 'RandomForest', 'Boosting']\n",
    "y1 = [model_train_rmse, model2_train_rmse, model3_train_rmse]\n",
    "y2 = [model_cv_rmse, model2_cv_rmse, model3_cv_rmse]\n",
    "\n",
    "plt.plot(x, y2, label = \"10-CV RMSE\", c='r')\n",
    "plt.plot(x, y1, label = \"Train RMSE\", c='b')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "998c911629ba937bcf1bf80465453e12e8c5c2c818cb936f93ef7cf495a937a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
